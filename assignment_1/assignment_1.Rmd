---
title: | 
  | Econometrics III
  | Assignment Part 1 & 2
  | Tinbergen Insitute
author: |
  | Stanislav Avdeev \hspace{3em} Bas Machielsen 
  | 590050sa \hspace{5em} 590049bm 
  | stnavdeev@gmail.com \hspace{2em} 590049bm@eur.nl
date: \today
output: 
  pdf_document: 
    latex_engine: lualatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  # Если мы не хотим код-чанков, можем менять на echo = FALSE
                      warning = FALSE,
                      message = FALSE,
                      out.width = "250pt", out.height = "200pt", 
                      fig.align = "center")

library(rio); library(tidyverse)
library(dynlm) ; library(stargazer)
library(normtest); library(reticulate)
library(AER)

```

<!-- I created a file .Renviron, where the python distribution is located on my system. You can find that by opening a terminal, and entering `$ which -a python python3`. Then, in RStudio, use `usethis::edit_r_environ()`, and add `RETICULATE_PYTHON="/Users/basmachielsen/opt/anaconda3/bin/python"` (or your directory) on a new line to the file. In this way, we can seamlessly interchange R and Python code chunks. Restart RStudio, and then everything is ready to go: --> 

## Question 1

**Part (a)**

```{r plotgdp}
# Part 1: Plot the Dutch GDP, ACF, and PACF
df <- readr::read_csv("./data/data_assign_p1.csv")

df <- ts(df$GDP_QGR, 
         frequency = 4, 
         start = c(1987, 2))
```

In the above figure, we observe the growth rates of Dutch GDP from 1987 to 2009. 

```{r plotacfpacf}
autoc <- acf(df, 
    lag.max = 12, plot = F) 

Box.test(df, 
         lag = 12, 
         type = "Ljung-Box")

pautoc <- pacf(df, 
     lag.max = 12, plot = F) 

```

In the above figure, we plot the ACF and PACF, which is the ACF controlled for the other lagged correlations. The ACF tells us that the correlation of GDP growth with its lags is very low - hinting at very little time-dependence in this time-series. More precisely, the estimated correlation coefficients are not higher than 0.2 (for the lag of 1 period). If the GDP is indeed generated by an AR(p) process, the estimates show that the process has low $\phi$'s (in absolute value), indicating a low time dependence. 

**Part (b)**

```{r, results='asis'}
ar4 <- dynlm(df ~ L(df, 1) + L(df, 2) + L(df, 3) + L(df, 4))
ar3 <- dynlm(df ~ L(df, 1) + L(df, 3) + L(df, 4))
ar2 <- dynlm(df ~ L(df, 1) + L(df, 3))
ar1 <- dynlm(df ~ L(df, 1))

ar1_m <- forecast::Arima(df, c(1,0,0), method = "CSS")

stargazer(ar4, ar3, ar2, ar1, 
          type = "latex", 
          header = FALSE,
          font.size = "small")

```

The table above shows us the coefficients of the $AR(p)$ models with 1 to 4 lags included. Using a general-to-specific approach, we eliminate insignificant lags ($\alpha = 0.05$ level) at each step. Column (1) shows that only the first lag is significant at the 0.1 level. The second lag has the lowest $\psi$ coefficient and the highest s.e., thus, we eliminate it first. Doing this iteratively, we are left with the first lag in the model which is significant at the 0.05 level. Investigating the coefficient, we observe there is a 1-period persistence of the time series on its lagged value. The _degree_ of persistence, however, is not large, as the magnitude of the coefficient is fairly small (0.26). 

```{r figure, echo = FALSE}
par(mfrow=c(2,2))

plot(df, 
     xlab = "Time", 
     ylab = "GDP Growth (%)", 
     main = "GDP Growth over Time")

plot(autoc, 
     main = "Autocorrelation Function")

plot(pautoc, 
     main = "Partial Autocorrelation Function")

plot(autocred, main = "Autocorrelation Function of Residuals", 
           xlab = "Time", 
           ylab = "Autocorrelation")

```

**Part c**

```{r}
# Part 3: Plot ACF of residuals
autocred <- acf(ar1_m$resid, 12, 
                plot = FALSE)

durbinWatsonTest(ar1)

```

In the fourth plot above, we show the autocorrelation of the residuals. We observe that none of the coefficients attain significance. Hence, we think that the model is well-specified. Formally, we compute the Durbin-Watson test statistic, of which the output is shown above. The null hypothesis of no autocorrelation is not rejected. In an unreported plot, we also investigate the _partial_ autocorrelation of residuals, in which there is also no sign of autocorrelation. 


**Part d**

```{r}
# Part 4: Forecast AR model for 2 years
df_pred <- predict(ar1_m, n.ahead = 8)$pred

```

We derived the forecasts by making use of the conditional expectation as the forecast minimizing the forecast error under quadratic loss. The conditional expectation of the forecast for $T+1$ is: $\mathbb{E}[X_{T+1} | X_1, \dots, X_T] = \mathbb{E}[\phi X_T + \alpha + \epsilon_{T+1} | X_1, \dots, X_T] = \phi X_T + \alpha$. Similarly, the forecast for $T+2$ equals $\phi \mathbb{E}[X_{T+1} + \alpha] = \phi (\alpha + \phi X_T) + \alpha$. Generalizing this pattern, we then end up with the following recursive forecasts (for $j > 1$):

$$
\mathbb{E}[X_{T+j}] = \alpha + \alpha \cdot \left( \sum_{i = 1}^{j-1} \phi^i \right) + \phi^j X^T
$$

**Part e**

```{r}
# Part 5: Produce CI
df_ciu <- predict(ar1_m, n.ahead = 8)$pred + predict(ar1_m, n.ahead = 8)$se*1.96
df_cil <- predict(ar1_m, n.ahead = 8)$pred - predict(ar1_m, n.ahead = 8)$se*1.96
```

**Part f**

```{r}
# Part 6: Check normality
jb.norm.test(ar1_m$resid)
# reject H0, innovations are not normally distributed
```

```{r}
true <- ts(c(0.49, -0.31, -2.7, -1.63, 0.28, 0.33, 0.66, 1.59, 0.51, 0.71, 0.81), 
         frequency = 4, 
         start = c(2009, 2))
ts.plot(df)
#points(true, type = "l", col = 'blue', lty = 1)
points(df_pred, type = "l", col = 'red', lty = 1)
points(df_ciu, type = "l", col = 'red', lty = 2)
points(df_cil, type = "l", col = 'red', lty = 2)
```

## Question 2

```{r}
# Get the data
df2 <- readr::read_csv("./data/data_assign_p2.csv")

df2 <- ts(df2[,2:3], 
         frequency = 4, 
         start = c(1987, 2))
```

**Part (a)**

```{r}
# Part a: Plot the Dutch GDP/unemployment, AR and ADL
plot(df2)
ar4 <- dynlm(df2[,1] ~ L(df2[,1], 1) + L(df2[,1], 2) + L(df2[,1], 3) + L(df2[,1], 4))
ar3 <- dynlm(df2[,1] ~ L(df2[,1], 1) + L(df2[,1], 3) + L(df2[,1], 4))
ar2 <- dynlm(df2[,1] ~ L(df2[,1], 1) + L(df2[,1], 3))

# Create lags
library(data.table)
df_lags <- setDT(as.data.frame(df2[,1]))[, paste("x", 1:4) := shift(x, 1:4)][]
df_lags2 <-  setDT(as.data.frame(df2[,2]))[, paste("x", 1:4) := shift(x, 1:4)][]
df_lags <- cbind(df_lags, df_lags2)
df_lags <- df_lags[,c(1:5, 7:10)]
colnames(df_lags) <- as.character(c(0:8))

# To check whether we can use lm and not dynlm with lags 
lm_1 <- lm(df_lags$`0` ~ df_lags$`1` + df_lags$`2` + df_lags$`3` + df_lags$`4` + df_lags$`5` + df_lags$`6` + df_lags$`7` + df_lags$`8`, df_lags)

adl8 <- dynlm(df2[,1] ~ L(df2[,1], 1) + L(df2[,1], 2) + L(df2[,1], 3) + L(df2[,1], 4) + L(df2[,2], 1) + L(df2[,2], 2) + L(df2[,2], 3) + L(df2[,2], 4))

stargazer(adl8, lm_1, type = "text", header = FALSE)
# yes, we can
```

